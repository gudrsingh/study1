Here’s a detailed explanation of Apache Airflow with examples, including sample DAGs and a data pipeline example.

Example 1: Simple DAG
A simple DAG that runs two tasks sequentially could look like this:

python

Copy
from airflow import DAG
from airflow.operators.dummy_operator import DummyOperator
from airflow.operators.python_operator import PythonOperator
from datetime import datetime

def print_hello():
    print("Hello, World!")

default_args = {
    'owner': 'airflow',
    'start_date': datetime(2023, 1, 1),
}

dag = DAG('simple_dag', default_args=default_args, schedule_interval='@daily')

start = DummyOperator(task_id='start', dag=dag)

hello_task = PythonOperator(
    task_id='hello_task',
    python_callable=print_hello,
    dag=dag,
)

end = DummyOperator(task_id='end', dag=dag)

start >> hello_task >> end
Example 2: Data Pipeline
Now, let’s create a more detailed data pipeline example that extracts data from an S3 bucket, transforms it, and then loads it into a database (e.g., Amazon Redshift).

Pipeline Overview
Extract: Download a CSV file from an S3 bucket.
Transform: Process the data (e.g., clean, filter).
Load: Load the processed data into Amazon Redshift.
Sample DAG Code
python

Copy
from airflow import DAG
from airflow.operators.s3_to_redshift_operator import S3ToRedshiftOperator
from airflow.operators.python_operator import PythonOperator
from airflow.hooks.S3_hook import S3Hook
from airflow.hooks.postgres_hook import PostgresHook
from datetime import datetime
import pandas as pd

default_args = {
    'owner': 'airflow',
    'start_date': datetime(2023, 1, 1),
}

dag = DAG('data_pipeline', default_args=default_args, schedule_interval='@daily')

# Step 1: Extract data from S3
def extract_data_from_s3(bucket_name, file_key):
    s3 = S3Hook()
    data = s3.read_key(file_key, bucket_name)
    with open('/tmp/data.csv', 'w') as f:
        f.write(data)

extract_task = PythonOperator(
    task_id='extract_from_s3',
    python_callable=extract_data_from_s3,
    op_kwargs={'bucket_name': 'my-bucket', 'file_key': 'data/raw_data.csv'},
    dag=dag,
)

# Step 2: Transform the data
def transform_data():
    df = pd.read_csv('/tmp/data.csv')
    # Example transformation: filter out rows with null values
    df_cleaned = df.dropna()
    df_cleaned.to_csv('/tmp/cleaned_data.csv', index=False)

transform_task = PythonOperator(
    task_id='transform_data',
    python_callable=transform_data,
    dag=dag,
)

# Step 3: Load data into Redshift
load_task = S3ToRedshiftOperator(
    task_id='load_to_redshift',
    schema='public',
    table='cleaned_data',
    s3_bucket='my-bucket',
    s3_key='data/cleaned_data.csv',
    copy_options=['CSV'],
    aws_conn_id='aws_default',
    redshift_conn_id='redshift_default',
    dag=dag,
)

# Set task dependencies
extract_task >> transform_task >> load_task
Explanation of the Pipeline
Extract Task:

Uses the S3Hook to read a CSV file from an S3 bucket and saves it to a temporary location (/tmp/data.csv).
Transform Task:

Reads the extracted CSV file into a Pandas DataFrame, performs a transformation (removing rows with null values), and saves the cleaned data back to a new CSV file (/tmp/cleaned_data.csv).
Load Task:

Uses the S3ToRedshiftOperator to load the cleaned data from S3 into an Amazon Redshift table. It specifies the schema, table name, S3 bucket, and key.
Running the Pipeline
Deploy the DAG: Save the DAG code in a Python file (e.g., data_pipeline.py) in the Airflow DAGs folder.

Start Airflow: Ensure that your Airflow environment is up and running.

Trigger the DAG: You can manually trigger the DAG from the Airflow UI or wait for the scheduled interval.